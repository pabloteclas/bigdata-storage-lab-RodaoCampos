# bigdata-storage-lab-RodaoCampos
# De CSVs heterog√©neos a un almac√©n anal√≠tico confiable  
**Laboratorio T√©cnico ‚Äî `bigdata-storage-lab-<apellido>`**

---

## 1. Objetivo

El prop√≥sito de este laboratorio es **dise√±ar e implementar un flujo completo de ingesta, validaci√≥n y normalizaci√≥n de datos heterog√©neos en CSV** para consolidarlos en un **almac√©n anal√≠tico confiable**.  

La secuencia de trabajo ser√°:

1. **Ingesta:** lectura y almacenamiento inicial de m√∫ltiples CSV (estructuras dispares, columnas faltantes, diferentes codificaciones/formatos).  
2. **Validaci√≥n:** aplicaci√≥n de reglas de calidad (tipos de datos, valores nulos, rangos v√°lidos, duplicados).  
3. **Normalizaci√≥n:** unificaci√≥n de esquemas, tipos de datos y claves de integraci√≥n.  
4. **Zonas de almacenamiento (Medallion Architecture):**  
   - **Bronze:** datos crudos tal como se ingieren.  
   - **Silver:** datos limpios y normalizados.  
   - **Gold:** modelos listos para anal√≠tica y generaci√≥n de KPIs.  
5. **KPIs:** c√°lculo de m√©tricas simples a partir de los datos consolidados (ej. volumen total de registros v√°lidos, % de errores, tendencias agregadas).

---

## 2. Entregables

- **Repositorio GitHub p√∫blico (`bigdata-storage-lab-<apellido>`):**  
  - C√≥digo fuente (Python/SQL/ETL).  
  - Scripts/notebooks de validaci√≥n y normalizaci√≥n.  
  - Estructura de carpetas clara (bronze/silver/gold).  
  - Documentaci√≥n (`README.md`, diagramas, instrucciones de ejecuci√≥n).  

- **Aplicaci√≥n Streamlit:**  
  - Dashboard que muestre KPIs y permita navegar los datos en las diferentes capas.  
  - Visualizaciones b√°sicas (tablas, gr√°ficos de barras/l√≠neas/pastel seg√∫n pertinencia).  

---

## 3. Criterios de Evaluaci√≥n

1. **Dise√±o y justificaci√≥n:**  
   - Claridad en la arquitectura propuesta y sus decisiones.  
   - Uso de buenas pr√°cticas (nombres consistentes, modularidad).  

2. **Calidad de los datos:**  
   - Aplicaci√≥n de validaciones relevantes y registro de errores.  
   - Nivel de limpieza y consistencia logrado en Silver/Gold.  

3. **Trazabilidad y modelo de almac√©n de datos:**  
   - Evidencia de flujo claro Bronze ‚Üí Silver ‚Üí Gold.  
   - Capacidad de reconstruir pasos de transformaci√≥n.  

4. **Documentaci√≥n:**  
   - Instrucciones reproducibles (setup, dependencias, ejecuci√≥n).  
   - Breve memoria t√©cnica justificando decisiones.  

---

## 4. Qu√© **NO** subir

- **Datos sensibles, confidenciales o con informaci√≥n personal identificable (PII).**  
- Archivos CSV reales de empresas/instituciones.  
- Claves de acceso, contrase√±as, tokens o credenciales en el repo o app.  

*(Se recomienda generar datos sint√©ticos o usar datasets p√∫blicos abiertos.)*

---

## 5. Tiempo estimado por fase

| Fase                        | Tiempo estimado |
|-----------------------------|-----------------|
| Ingesta de CSVs             | 2 h             |
| Validaci√≥n de calidad       | 3 h             |
| Normalizaci√≥n y esquemas    | 4 h             |
| Dise√±o Bronze/Silver/Gold   | 3 h             |
| Desarrollo de KPIs          | 3 h             |
| Implementaci√≥n en Streamlit | 3 h             |
| Documentaci√≥n final         | 2 h             |
| **Total aproximado**        | **20 h**        |

---

üìå **Entrega final:** subir el repo en GitHub p√∫blico (`bigdata-storage-lab-<apellido>`) e incluir el link a la app de Streamlit desplegada.

## üèÜ Retos opcionales (dificultad creciente)

### üîπ Reto A: Validaciones extra
- **Objetivo:** Asegurar calidad de datos mediante reglas adicionales.  
- **Pasos sugeridos:**  
  1. En `validate.py`, implementar control de rangos de fechas (ej. fecha ‚â• 2015).  
  2. Detectar duplicados seg√∫n clave natural (ej. `id_cliente + fecha`).  
  3. Rechazar filas no v√°lidas y registrarlas en un log.  
- **Criterio de aceptaci√≥n:**  
  - En el repo se ve la funci√≥n extra en `validate.py`.  
  - La app muestra conteo de registros rechazados.  

---

### üîπ Reto B: M√©trica de veracidad en la app
- **Objetivo:** Calcular un % de registros v√°lidos por archivo y mostrarlo en el dashboard.  
- **Pasos sugeridos:**  
  1. Modificar `validate.py` para devolver (v√°lidos, inv√°lidos).  
  2. Calcular `veracidad = v√°lidos / total * 100`.  
  3. En `streamlit_app.py`, a√±adir un gr√°fico simple (ej. barras).  
- **Criterio de aceptaci√≥n:**  
  - Al subir un CSV, la app muestra la m√©trica % por archivo.  
  - Evidencia en README (captura).  

---

### üîπ Reto C: Tabla Gold con linaje
- **Objetivo:** Crear una tabla optimizada para reporting y mantener trazabilidad.  
- **Pasos sugeridos:**  
  1. Definir tabla **Gold**: ej. ventas mensuales por categor√≠a (`fecha, categoria, ventas_total`).  
  2. Documentar en README c√≥mo se genera desde Silver.  
  3. A√±adir metadato de linaje (ej. en columna `origen_archivo`).  
- **Criterio de aceptaci√≥n:**  
  - Archivo `gold.csv` generado en `/data/gold/`.  
  - README explica reglas de derivaci√≥n y linaje.  

---

### üîπ Reto D: Ensayo CSV vs Parquet
- **Objetivo:** Comparar formatos y concluir cu√°l es m√°s eficiente en este caso.  
- **Pasos sugeridos:**  
  1. Generar dataset sint√©tico de al menos 100k filas.  
  2. Guardar como CSV y Parquet.  
  3. Medir: tama√±o en disco y tiempo de lectura con pandas.  
  4. Escribir conclusiones en README.  
- **Criterio de aceptaci√≥n:**  
  - Notebook o script con el experimento incluido en `src/`.  
  - Resultados (tabla comparativa) documentados en README.  

---

### üîπ Reto E: Simulaci√≥n paso a streaming
- **Objetivo:** Analizar c√≥mo evolucionar√≠a la arquitectura hacia streaming.  
- **Pasos sugeridos:**  
  1. Identificar qu√© capa cambia primero (ej. ingesta ‚Üí micro-batches a colas).  
  2. Dise√±ar esquema de flujo actualizado (diagrama simple en `docs/`).  
  3. Redactar pseudoc√≥digo de ingesta en tiempo real (ej. usando colas + validaci√≥n online).  
- **Criterio de aceptaci√≥n:**  
  - Diagrama en `docs/streaming_design.png` o `.md`.  
  - Pseudoc√≥digo documentado en README.  
  - Justificaci√≥n de la capa que se adapta primero.  


## üöÄ Pega y corre ‚Äî Gu√≠a r√°pida de entrega

### Pasos m√≠nimos (en orden)

1. **Crear repositorio con estructura**  
   - Nombre: `bigdata-storage-lab-<apellido>`.  
   - Crear carpetas (`src`, `data/raw`, `data/bronze`, `data/silver`, `data/gold`, `docs`, `tests`).  
   - Tiempo estimado: **20 min**.  
   - ‚úîÔ∏è Verificar: que todas las carpetas aparecen en GitHub con `.gitkeep`.

2. **Pegar archivos base**  
   - Subir `ingest.py`, `validate.py`, `transform.py`, `streamlit_app.py`, `requirements.txt`.  
   - Tiempo estimado: **30 min**.  
   - ‚úîÔ∏è Verificar: el repo abre bien los archivos y no hay errores de sintaxis al ejecutar `streamlit run streamlit_app.py` en local.

3. **Subir CSVs de prueba a `/data/raw/`**  
   - Usar datasets sint√©ticos (ej. ventas, IoT, fraude).  
   - Tiempo estimado: **15 min**.  
   - ‚úîÔ∏è Verificar: que la app procesa al menos 2‚Äì3 archivos distintos y genera Bronze + Silver.

4. **Desplegar en Streamlit Community Cloud**  
   - Conectar el repo, elegir `streamlit_app.py` como archivo principal.  
   - Tiempo estimado: **30 min**.  
   - ‚úîÔ∏è Verificar: la URL abre y se pueden subir CSVs desde el navegador.

5. **Completar README con reflexiones**  
   - A√±adir secci√≥n ‚Äúüìù Prompts de reflexi√≥n‚Äù y responder los 5 puntos.  
   - Tiempo estimado: **20 min**.  
   - ‚úîÔ∏è Verificar: README tiene arquitectura, justificaci√≥n (5V), checklist, capturas en `docs/`.

6. **Entregar URLs**  
   - **Repo p√∫blico en GitHub.**  
   - **URL de la app en Streamlit.**  
   - Tiempo estimado: **5 min**.  
   - ‚úîÔ∏è Verificar: ambos links son accesibles sin credenciales.

---

### ‚è±Ô∏è Tiempo total estimado: **~2 h**

### ‚úÖ Checklist final antes de entregar
- [ ] URL de Streamlit funcional.  
- [ ] `bronze.csv` y `silver.csv` generados en `/data`.  
- [ ] README con decisiones justificadas + reflexiones respondidas.  
- [ ] Diccionario y gobernanza completos en `docs/`.  
- [ ] Capturas de pantallas del dashboard incluidas.  
- [ ] Tests/checklist.md marcado.  


### üìù Prompts de reflexi√≥n ‚Äî Ejemplo de respuestas completas

1. **Pregunta:** V dominante hoy y V dominante si se duplica el tr√°fico (justifica en 3 l√≠neas)  
   **Respuesta:**  
   Hoy la V dominante es **Velocidad**, porque necesitamos procesar los CSV y micro-batches r√°pidamente.  
   Si el tr√°fico se duplica, la V dominante ser√≠a **Volumen**, ya que el n√∫mero de registros aumentar√≠a significativamente.  
   Esto impacta en almacenamiento y procesamiento, requiriendo ajustes de pipeline.

2. **Pregunta:** Trade-off elegido (ej.: m√°s compresi√≥n vs CPU): por qu√© lo elegiste y c√≥mo lo medir√°s  
   **Respuesta:**  
   Elegimos compresi√≥n moderada (**Parquet con snappy**) para equilibrar tama√±o de almacenamiento y CPU.  
   Esto reduce espacio sin afectar demasiado el tiempo de lectura.  
   Mediremos impacto con benchmarks de ingesti√≥n y consultas en Silver.

3. **Pregunta:** Por qu√© ‚Äúinmutable + linaje‚Äù mejora veracidad y qu√© coste a√±ade  
   **Respuesta:**  
   Mantener datos **inmutables** y registrar **linaje** asegura que siempre podemos rastrear cada registro hasta su origen, aumentando la confianza en los datos.  
   El coste a√±adido es **mayor consumo de almacenamiento** y algo de overhead en metadatos.

4. **Pregunta:** KPI principal y SLA del dashboard: qu√© decisi√≥n habilita y por qu√© esa latencia  
   **Respuesta:**  
   KPI principal: **volumen total de transacciones v√°lidas por mes**.  
   SLA: actualizaci√≥n en **<5 min** tras la subida de CSV.  
   Esta latencia permite an√°lisis casi en tiempo real sin sobrecargar el servidor.

5. **Pregunta:** Riesgo principal del dise√±o y mitigaci√≥n t√©cnica concreta  
   **Respuesta:**  
   Riesgo: **errores de encoding o formatos inconsistentes** en CSV de origen.  
   Mitigaci√≥n: normalizaci√≥n autom√°tica con **fallback UTF-8 ‚Üí Latin-1**, y validaciones en Bronze que registran fallos para poder corregirlos posteriormente.

